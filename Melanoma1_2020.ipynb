{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom skimage.transform import resize\nfrom skimage.measure import block_reduce\nimport tensorflow as tf\nfrom tensorflow.keras.applications import *\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, AveragePooling2D, Flatten, Dropout, Dense, concatenate\nimport keras\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, roc_curve, auc, accuracy_score\nfrom numpy import where\nimport tensorflow_addons as tfa\nfrom tensorflow.keras import Model, Input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!git clone https://github.com/rininobaron/Kaggle_Melanoma_2020.git","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv('./Kaggle_Melanoma_2020/train_sample.csv')\ntest=pd.read_csv('./Kaggle_Melanoma_2020/test_sample.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_images_to_np(df=pd.read_csv('./Kaggle_Melanoma_2020/test_sample.csv'), submission=False):\n    #Parameters\n    #df: Pandas DataFrame with the names of images\n    #submission:boolea, if True df is equal to pd.read_csv('../input/siim-isic-melanoma-classification/test.csv')\n    #\n    #Returns\n    #X: numpy array with images\n    \n    if submission:\n        df=pd.read_csv('../input/siim-isic-melanoma-classification/test.csv')\n        path_root='../input/siim-isic-melanoma-classification/jpeg/test/'\n    \n    path_root='../input/siim-isic-melanoma-classification/jpeg/train/'\n    \n    #Reference image to standardize reslution\n    img = mpimg.imread(path_root+'ISIC_0015719.jpg')\n    shape1=img.shape[0]\n    shape2=img.shape[1]\n    \n    #Our image is very large then we propose reduce the resolution iimage by a factor_reduce\n    factor_reduce=25\n    X = np.zeros((df['image_name'].count(),int(shape1/factor_reduce),int(shape2/factor_reduce),img.shape[2]))\n    \n    count=0\n    for i in df['image_name']:\n        \n        print('Processing image in row: '+str(count))\n        \n        #Getting image\n        img = mpimg.imread(path_root+i+'.jpg')\n        print(type(np.max(img)))\n        \n        #sns.distplot(img[:,:,0],kde=False, rug=True)\n        #a=plt.hist(img[:,:,2].ravel(), bins=256, range=(0.0, 255.0), fc='k', ec='k')\n        #plt.show(a)\n        \n        #Ensure standardize reslution\n        if shape1!=img.shape[0] or shape2!=img.shape[1]:\n            img=resize(img,(shape1,shape2))\n            #Convert the image to a 0-255 scale.\n            img=255*img\n            #Convert to integer data type pixels.\n            img = img.astype(np.uint8)\n        \n        #sns.distplot(img[:,:,0],kde=False, rug=True)\n        #b=plt.hist(img[:,:,2].ravel(), bins=256, range=(0.0, 255.0), fc='k', ec='k')\n        #plt.show(b)\n        \n        #Force image to uint8\n        #img=img_as_ubyte(img)\n        \n        #Display important values to track function behavior\n        print(np.max(img))\n        print(type(np.max(img)))\n        \n        #Display the first four images in train\n        if count<=3:\n            print('Image name is: '+i)\n            temp=plt.imshow(img, vmin=0,vmax=255)\n            plt.show(temp)\n        \n        #But the image is very large yet\n        #Therefore we propose to reduce it using from skimage.measure.block_reduce\n        #and np.mean\n        img = block_reduce(img, block_size=(factor_reduce, factor_reduce, 1), func=np.mean)\n        \n        #Convert to integer data type pixels.\n        img = img.astype(np.uint8)\n        \n        #Saving img in X array\n        \n        X[count,:,:,:]=img\n        \n        #Display the first four images in train\n        if count<=3:\n            print('Image name reduce by factor '+str(factor_reduce)+' is: '+i)\n            temp=plt.imshow(img, vmin=0,vmax=255)\n            plt.show(temp)\n        \n        count+=1\n        \n        \n    return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test=get_images_to_np(df=test, submission=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train=get_images_to_np(df=train, submission=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Setting true outputs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train=train['target'].to_numpy()\nY_test=test['target'].to_numpy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Setting weights per class to try to fix unbalanced class problem","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"w_p=np.sum(Y_train==0)/Y_train.shape[0]\nw_n=np.sum(Y_train==1)/Y_train.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating dictionary\nclass_weights = { 0 : w_n , 1 : w_p }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defining Important parameters for training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# parameters\nEPOCHS = 200\nINIT_LR = 1e-4\nBS = X_train.shape[0]\nCLASSES = 1 #Very important number\n#norm_size = 28\n\nopt=tf.keras.optimizers.Nadam(lr=INIT_LR, decay=INIT_LR / EPOCHS, clipvalue=5) #Setting clipvalue to avoid exploding gradientes\n#opt=tfa.optimizers.LAMB(learning_rate=INIT_LR, decay= INIT_LR / EPOCHS,clipvalue=5)\n\n# start to train model\nprint('start to train model')\n\n#Setting Metrics\nMETRICS = [\n  tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n  tf.keras.metrics.Precision(name='precision'),\n  tf.keras.metrics.Recall(name='recall'),\n  tf.keras.metrics.AUC(name='AUC')\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Output Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Global Constants\nWEIGHT_DECAY=0.005\nLRN2D_NORM=True\nDATA_FORMAT='channels_last' # Theano:'channels_first' Tensorflow:'channels_last'\nUSE_BN=True\n\ndef conv2D_lrn2d(x,filters,kernel_size,strides=(1,1),padding='same',dilation_rate=(1,1),activation='relu',\n                 use_bias=True,kernel_initializer='he_normal',bias_initializer='zeros',\n                 kernel_regularizer=None,bias_regularizer=None,activity_regularizer=None,\n                 kernel_constraint=None,bias_constraint=None,lrn2d_norm=LRN2D_NORM,weight_decay=WEIGHT_DECAY):\n    #l2 normalization\n    if weight_decay:\n        kernel_regularizer=regularizers.l2(weight_decay)\n        bias_regularizer=regularizers.l2(weight_decay)\n    else:\n        kernel_regularizer=None\n        bias_regularizer=None\n    x=Conv2D(filters=filters,kernel_size=kernel_size,strides=strides,padding=padding,dilation_rate=dilation_rate,\n             activation=activation,use_bias=use_bias,kernel_initializer=kernel_initializer,\n             bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,\n             activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(x)\n    if lrn2d_norm:\n        #batch normalization\n        x=BatchNormalization()(x)\n\n    return x\n\ndef inception_module(x,params,concat_axis,padding='same',dilation_rate=(1,1),activation='relu',\n                     use_bias=True,kernel_initializer='he_normal',bias_initializer='zeros',\n                     kernel_regularizer=None,bias_regularizer=None,activity_regularizer=None,kernel_constraint=None,\n                     bias_constraint=None,lrn2d_norm=LRN2D_NORM,weight_decay=None):\n    (branch1,branch2,branch3,branch4)=params\n    if weight_decay:\n        kernel_regularizer=regularizers.l2(weight_decay)\n        bias_regularizer=regularizers.l2(weight_decay)\n    else:\n        kernel_regularizer=None\n        bias_regularizer=None\n    #1x1\n    pathway1=Conv2D(filters=branch1[0],kernel_size=(1,1),strides=1,padding=padding,dilation_rate=dilation_rate,\n                    activation=activation,use_bias=use_bias,kernel_initializer=kernel_initializer,\n                    bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,\n                    activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(x)\n    #1x1->3x3\n    pathway2=Conv2D(filters=branch2[0],kernel_size=(1,1),strides=1,padding=padding,dilation_rate=dilation_rate,\n                    activation=activation,use_bias=use_bias,kernel_initializer=kernel_initializer,\n                    bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,\n                    activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(x)\n    pathway2=Conv2D(filters=branch2[1],kernel_size=(3,3),strides=1,padding=padding,dilation_rate=dilation_rate,\n                    activation=activation,use_bias=use_bias,kernel_initializer=kernel_initializer,\n                    bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,\n                    activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(pathway2)\n    #1x1->5x5\n    pathway3=Conv2D(filters=branch3[0],kernel_size=(1,1),strides=1,padding=padding,dilation_rate=dilation_rate,\n                    activation=activation,use_bias=use_bias,kernel_initializer=kernel_initializer,\n                    bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,\n                    activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(x)\n    pathway3=Conv2D(filters=branch3[1],kernel_size=(5,5),strides=1,padding=padding,dilation_rate=dilation_rate,\n                    activation=activation,use_bias=use_bias,kernel_initializer=kernel_initializer,\n                    bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,\n                    activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(pathway3)\n    #3x3->1x1\n    pathway4=MaxPooling2D(pool_size=(3,3),strides=1,padding=padding,data_format=DATA_FORMAT)(x)\n    pathway4=Conv2D(filters=branch4[0],kernel_size=(1,1),strides=1,padding=padding,dilation_rate=dilation_rate,\n                    activation=activation,use_bias=use_bias,kernel_initializer=kernel_initializer,\n                    bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,\n                    activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(pathway4)\n\n    return concatenate([pathway1,pathway2,pathway3,pathway4],axis=concat_axis)\n\nclass inceptionV3:\n    @staticmethod\n    def build(width, height, depth, NB_CLASS):\n        INP_SHAPE = (height, width, depth)\n        img_input = Input(shape=INP_SHAPE)\n        CONCAT_AXIS = 3\n        \n        x = inception_module(img_input, params=[(64,), (96, 128), (16, 32), (32,)], concat_axis=CONCAT_AXIS)  # 3a\n\n        # Create a Keras Model\n        model = Model(inputs=img_input, outputs=[x])\n        model.summary()\n        # Save a PNG of the Model Build\n        #plot_model(model, to_file='../imgs/GoogLeNet.png')\n        # return the constructed network architecture\n        return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Setting baseline model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rnet = InceptionV3(weights='imagenet', include_top=False ,input_shape=[X_train.shape[1],X_train.shape[2],X_train.shape[3]])    \nrnet.trainable=False\nrnet.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_model = tf.keras.Sequential([rnet])\nnew_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = inceptionV3.build(width=3, height=6, depth=2048, NB_CLASS=100) \nmodel2.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Merging models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"new_model.add(model2)\nnew_model.add(tf.keras.layers.GlobalAveragePooling2D())\nnew_model.add(Dropout(0.4)) #In order to avoid overfitting\n#To avoid overfitting kernel regularizer was setting with two parameters\nnew_model.add(Dense(units=CLASSES, activation='sigmoid',kernel_regularizer=keras.regularizers.l2(l=0.1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Baseline Model Architecture","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"BASELINE MODEL ARCHITECTURE\")\nrnet.summary()\nprint()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final Model Architecture","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"FINAL MODEL TO TRAINING AND TESTING\")\nnew_model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Compiling Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"new_model.compile(\n    optimizer=opt,\n    loss = 'binary_crossentropy',\n    metrics=METRICS\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Use Data Augmentation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1,\n                             height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n                             horizontal_flip=True, fill_mode=\"nearest\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"history_1 = new_model.fit(aug.flow(X_train, Y_train, batch_size=BS),\n                            steps_per_epoch=len(X_train) // BS, validation_data=aug.flow(X_test, Y_test),\n                            epochs=EPOCHS, verbose=2, class_weight=class_weights)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Setting important function to visualize metrics","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Setting function to display confusion matrix\ndef display_confusion_matrix(cmat, score, precision, recall):   ###1\n    plt.figure(figsize=(15,15))\n    ax = plt.gca()\n    ax.matshow(cmat, cmap='Reds')\n    ax.set_xticks(range(CLASSES+1))\n    ax.set_xticklabels([\"0\",\"1\"], fontdict={'fontsize': 34})\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"left\", rotation_mode=\"anchor\")\n    ax.set_yticks(range(CLASSES+1))\n    ax.set_yticklabels([\"0\",\"1\"], fontdict={'fontsize': 34})\n    plt.setp(ax.get_yticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n    titlestring = \"\"\n    if score is not None:\n        titlestring += 'f1 = {:.3f} '.format(score)\n    if precision is not None:\n        titlestring += '\\nprecision = {:.3f} '.format(precision)\n    if recall is not None:\n        titlestring += '\\nrecall = {:.3f} '.format(recall)\n    if len(titlestring) > 0:\n        ax.text(1.25, -0.25, titlestring, fontdict={'fontsize': 22, 'horizontalalignment':'right', 'verticalalignment':'top', 'color':'#000000'})\n    plt.title(\"Confusion Matrix\", fontdict={'fontsize': 34})\n    plt.savefig('Matriz_de_Confusion.png')\n    plt.show()\n    \n\n#Setting function to display training curves\ndef display_training_curves(training, validation, title, subplot):   ####2\n    if subplot%10==1: # set up the subplots on the first call\n        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n        plt.tight_layout()\n    ax = plt.subplot(subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(training)\n    ax.plot(validation)\n    ax.set_title('model '+ title)\n    ax.set_ylabel(title)\n    #ax.set_ylim(0.28,1.05)\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'valid.'])\n    plt.savefig(str(title)+'.png')\n\n#Setting function to display ROC curve\ndef plot_roc(name, labels, predictions):\n    fp, tp, _ = roc_curve(labels, predictions)\n    auc1 = auc(fp, tp)\n    lw=2\n\n    plt.plot(100*fp, 100*tp, linewidth=2, lw=lw, color='darkorange', label='ROC curve (area = %0.2f)' % auc1)\n    plt.plot([0, 100], [0, 100], color='navy', lw=lw, linestyle='--',label='Random')\n    plt.xlabel('False positives [%]')\n    plt.ylabel('True positives [%]')\n    plt.xlim([-5,105])\n    plt.ylim([-5,105])\n    plt.grid(True)\n    ax = plt.gca()\n    ax.set_aspect('equal')\n    plt.title(name)\n    plt.legend(loc=\"lower right\")\n    plt.savefig('ROC.png')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plotting Validation Metrics","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Setting class predict vectors\ny_train_pred_labeled=(new_model.predict(X_train) > 0.5).astype(\"int32\")\ny_test_pred_labeled=(new_model.predict(X_test) > 0.5).astype(\"int32\")\nprint()\nprint('y_test_pred_labeled:')\nprint(y_test_pred_labeled)\nprint()\n\n#Setting validation metrics\nprint('Prediction probabilities: '+str(new_model.predict(X_test)))\nprint()\nprint()\n#METRICS FROM FINAL WEIGHTS MODEL\ncmat = confusion_matrix(Y_test, y_test_pred_labeled)\nscore = f1_score(Y_test, y_test_pred_labeled)\nprecision = precision_score(Y_test, y_test_pred_labeled)\nrecall = recall_score(Y_test, y_test_pred_labeled)\n\n#Display training curves (loss & accuracy)\ndisplay_training_curves(history_1.history['loss'], history_1.history['val_loss'], 'loss', 211)\ndisplay_training_curves(history_1.history['accuracy'], history_1.history['val_accuracy'], 'accuracy', 212)\n\n#Display training curves (loss & accuracy)\nprecision_history1=np.array(history_1.history['precision'])\nprecision_history2=np.array(history_1.history['val_precision'])\n\nrecall_history1=np.array(history_1.history['recall'])\nrecall_history2=np.array(history_1.history['val_recall'])\n\nf1_history_train= 2*(precision_history1*recall_history1)/(precision_history1+recall_history1)\nf1_history_train[where(np.isnan(f1_history_train) == True)] = 0\nf1_history_test=2*(precision_history2*recall_history2)/(precision_history2+recall_history2)\nf1_history_test[where(np.isnan(f1_history_test) == True)] = 0\n\ndisplay_training_curves(f1_history_train, f1_history_test, 'f1_score', 211)\ndisplay_training_curves(history_1.history['AUC'], history_1.history['val_AUC'], 'auc', 212)\n\n#Display confusion matrix\ndisplay_confusion_matrix(cmat, score, precision, recall)   ####2\n\n#Display ROC\nplot_roc('ROC Thermy tests', Y_test, new_model.predict(X_test))\n\n#Getting max indez from f1_score \nindex=where(f1_history_test==np.max(f1_history_test))\nindex=index[0][0]\n\n#MAXIMUM metrics\nprint()\nprint()\nprint(\"MAXIMUM METRICS\")\nprint()\nprint(\"train_acc:\")\nprint(history_1.history['accuracy'][index])\nprint()\nprint(\"test_acc:\")\nprint(history_1.history['val_accuracy'][index])\nprint()\nprint(\"precision:\")\nprint(history_1.history['val_precision'][index])\nprint()\nprint(\"recall\")\nprint(history_1.history['val_recall'][index])\nprint()\nprint(\"f1_score\")\nprint(f1_history_test[index])\nprint()\nprint(\"AUC\")\nprint(history_1.history['val_AUC'][index])\nprint()\n\n\n#AVERAGE METRICS\nprint()\nprint(\"AVERAGE METRICS\")\nprint()\nprint(\"train_acc:\")\nprint(np.sum(history_1.history['accuracy'])/len(history_1.history['accuracy']))\nprint()\nprint(\"test_acc:\")\nprint(np.sum(history_1.history['val_accuracy'])/len(history_1.history['val_accuracy']))\nprint()\nprint(\"precision:\")\nprint(np.sum(history_1.history['val_precision'])/len(history_1.history['val_precision']))\nprint()\nprint(\"recall\")\nprint(np.sum(history_1.history['val_recall'])/len(history_1.history['val_recall']))\nprint()\nprint(\"f1_score\")\nprint(np.sum(f1_history_test)/len(f1_history_test))\nprint()\nprint(\"AUC\")\nprint(np.sum(history_1.history['val_AUC'])/len(history_1.history['val_AUC']))\nprint()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"hola\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}